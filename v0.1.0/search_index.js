var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = MCMCDiagnosticTools","category":"page"},{"location":"#MCMCDiagnosticTools","page":"Home","title":"MCMCDiagnosticTools","text":"","category":"section"},{"location":"#Effective-sample-size-and-potential-scale-reduction","page":"Home","title":"Effective sample size and potential scale reduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The effective sample size (ESS) and the potential scale reduction can be estimated with ess_rhat.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ess_rhat","category":"page"},{"location":"#MCMCDiagnosticTools.ess_rhat","page":"Home","title":"MCMCDiagnosticTools.ess_rhat","text":"ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real},3}; method=ESSMethod(), maxlag=250\n)\n\nEstimate the effective sample size and the potential scale reduction of the samples of shape (draws, parameters, chains) with the method and a maximum lag of maxlag.\n\nSee also: ESSMethod, FFTESSMethod, BDAESSMethod\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"The following methods are supported:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ESSMethod\nFFTESSMethod\nBDAESSMethod","category":"page"},{"location":"#MCMCDiagnosticTools.ESSMethod","page":"Home","title":"MCMCDiagnosticTools.ESSMethod","text":"ESSMethod <: AbstractESSMethod\n\nThe ESSMethod uses a standard algorithm for estimating the effective sample size of MCMC chains.\n\nIt is is based on the discussion by Vehtari et al. and uses the biased estimator of the autocovariance, as discussed by Geyer. In contrast to Geyer, the divisor n - 1 is used in the estimation of the autocovariance to obtain the unbiased estimator of the variance for lag 0.\n\nReferences\n\nGeyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483.\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved widehat R for assessing convergence of MCMC. Bayesian Analysis.\n\n\n\n\n\n","category":"type"},{"location":"#MCMCDiagnosticTools.FFTESSMethod","page":"Home","title":"MCMCDiagnosticTools.FFTESSMethod","text":"FFTESSMethod <: AbstractESSMethod\n\nThe FFTESSMethod uses a standard algorithm for estimating the effective sample size of MCMC chains.\n\nThe algorithm is the same as the one of ESSMethod but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation.\n\ninfo: Info\nTo be able to use this method, you have to load a package that implements the AbstractFFTs.jl interface such as FFTW.jl or FastTransforms.jl.\n\n\n\n\n\n","category":"type"},{"location":"#MCMCDiagnosticTools.BDAESSMethod","page":"Home","title":"MCMCDiagnosticTools.BDAESSMethod","text":"BDAESSMethod <: AbstractESSMethod\n\nThe BDAESSMethod uses a standard algorithm for estimating the effective sample size of MCMC chains.\n\nIt is is based on the discussion by Vehtari et al. and uses the variogram estimator of the autocorrelation function discussed by Gelman et al.\n\nReferences\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press.\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved widehat R for assessing convergence of MCMC. Bayesian Analysis.\n\n\n\n\n\n","category":"type"},{"location":"#Monte-Carlo-standard-error","page":"Home","title":"Monte Carlo standard error","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"mcse","category":"page"},{"location":"#MCMCDiagnosticTools.mcse","page":"Home","title":"MCMCDiagnosticTools.mcse","text":"mcse(x::AbstractVector{<:Real}; method::Symbol=:imse, kwargs...)\n\nCompute the Monte Carlo standard error (MCSE) of samples x. The optional argument method describes how the errors are estimated. Possible options are:\n\n:bm for batch means [Glynn1991]\n:imse initial monotone sequence estimator [Geyer1992]\n:ipse initial positive sequence estimator [Geyer1992]\n\n[Glynn1991]: Glynn, P. W., & Whitt, W. (1991). Estimating the asymptotic variance with batch means. Operations Research Letters, 10(8), 431-435.\n\n[Geyer1992]: Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483.\n\n\n\n\n\n","category":"function"},{"location":"#R-diagnostic","page":"Home","title":"R⋆ diagnostic","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"rstar","category":"page"},{"location":"#MCMCDiagnosticTools.rstar","page":"Home","title":"MCMCDiagnosticTools.rstar","text":"rstar(\n    rng=Random.GLOBAL_RNG,\n    classifier,\n    samples::AbstractMatrix,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.8,\n    verbosity::Int=0,\n)\n\nCompute the R^* convergence statistic of the samples with shape (draws, parameters) and corresponding chains chain_indices with the classifier.\n\nThis implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari.\n\nThe classifier has to be a supervised classifier of the MLJ framework (see the MLJ documentation for a list of supported models). It is trained with a subset of the samples. The training of the classifier can be inspected by adjusting the verbosity level.\n\nIf the classifier is deterministic, i.e., if it predicts a class, the value of the R^* statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the R^* statistic is returned (algorithm 2).\n\nnote: Note\nThe correctness of the statistic depends on the convergence of the classifier used internally in the statistic.\n\nExamples\n\njulia> using MLJBase, MLJXGBoostInterface, Statistics\n\njulia> samples = fill(4.0, 300, 2);\n\njulia> chain_indices = repeat(1:3; outer=100);\n\nOne can compute the distribution of the R^* statistic (algorithm 2) with the probabilistic classifier.\n\njulia> distribution = rstar(XGBoostClassifier(), samples, chain_indices);\n\njulia> isapprox(mean(distribution), 1; atol=0.1)\ntrue\n\nFor deterministic classifiers, a single R^* statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models.\n\njulia> @pipeline XGBoostClassifier name = XGBoostDeterministic operation = predict_mode;\n\njulia> value = rstar(XGBoostDeterministic(), samples, chain_indices);\n\njulia> isapprox(value, 1; atol=0.1)\ntrue\n\nReferences\n\nLambert, B., & Vehtari, A. (2020). R^*: A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers.\n\n\n\n\n\n","category":"function"},{"location":"#Other-diagnostics","page":"Home","title":"Other diagnostics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"discretediag\ngelmandiag\ngelmandiag_multivariate\ngewekediag\nheideldiag\nrafterydiag","category":"page"},{"location":"#MCMCDiagnosticTools.discretediag","page":"Home","title":"MCMCDiagnosticTools.discretediag","text":"discretediag(chains::AbstractArray{<:Real,3}; frac=0.3, method=:weiss, nsim=1_000)\n\nCompute discrete diagnostic where method can be one of :weiss, :hangartner, :DARBOOT, :MCBOOT, :billinsgley, and :billingsleyBOOT.\n\n\n\n\n\n","category":"function"},{"location":"#MCMCDiagnosticTools.gelmandiag","page":"Home","title":"MCMCDiagnosticTools.gelmandiag","text":"gelmandiag(chains::AbstractArray{<:Real,3}; alpha::Real=0.95)\n\nCompute the Gelman, Rubin and Brooks diagnostics.\n\n\n\n\n\n","category":"function"},{"location":"#MCMCDiagnosticTools.gelmandiag_multivariate","page":"Home","title":"MCMCDiagnosticTools.gelmandiag_multivariate","text":"gelmandiag_multivariate(chains::AbstractArray{<:Real,3}; alpha::Real=0.05)\n\nCompute the multivariate Gelman, Rubin and Brooks diagnostics.\n\n\n\n\n\n","category":"function"},{"location":"#MCMCDiagnosticTools.gewekediag","page":"Home","title":"MCMCDiagnosticTools.gewekediag","text":"gewekediag(x::AbstractVector{<:Real}; first::Real=0.1, last::Real=0.5, kwargs...)\n\nCompute the Geweke diagnostic from the first and last proportion of samples x.\n\n\n\n\n\n","category":"function"},{"location":"#MCMCDiagnosticTools.heideldiag","page":"Home","title":"MCMCDiagnosticTools.heideldiag","text":"heideldiag(\n    x::AbstractVector{<:Real}; alpha::Real=0.05, eps::Real=0.1, start::Int=1, kwargs...\n)\n\nCompute the Heidelberger and Welch diagnostic.\n\n\n\n\n\n","category":"function"},{"location":"#MCMCDiagnosticTools.rafterydiag","page":"Home","title":"MCMCDiagnosticTools.rafterydiag","text":"rafterydiag(x::AbstractVector{<:Real}; q, r, s, eps, range)\n\nCompute the Raftery and Lewis diagnostic.\n\n\n\n\n\n","category":"function"}]
}
